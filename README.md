# Голосовой ReAct-агент для управления роботом

Этот проект демонстрирует продвинутую архитектуру для создания голосового ассистента, управляющего роботизированным манипулятором. В основе системы лежит **ReAct-парадигма (Reason + Act)**, которая позволяет LLM-агенту не просто выполнять одиночные команды, а **рассуждать**, **строить многошаговые планы** и **адаптироваться** к результатам своих действий.

Система полностью асинхронна, что обеспечивает отзывчивость и возможность прерывания задач.

## Ключевые возможности
- **Голосовое управление:** Принимает команды голосом и озвучивает ответы.
- **ReAct-логика:** Агент может выполнять сложные задачи (например, "возьми кубик и переложи его"), разбивая их на последовательность простых действий (`подъехать`, `схватить`, `поднять`, `переместить`, `отпустить`).
- **Самокоррекция:** Если действие не удалось, агент получает сообщение об ошибке и может попытаться исправить ситуацию (например, вызвать другой инструмент).
- **Долговременная память:** Агент помнит весь контекст текущего диалога.
- **Конфигурируемость:** Озвучку можно отключить для быстрой отладки через переменную окружения.
- **Расширяемость:** Архитектура позволяет легко добавлять новые "инструменты" для робота.

---

## Архитектура: Цикл ReAct (Reason + Act)

В отличие от простого "переводчика" команд, эта система работает как настоящий мыслящий агент. Весь процесс взаимодействия построен вокруг цикла "Мысль -> Действие -> Наблюдение".

```mermaid
graph TD
    subgraph Пользователь
        A[Голосовая команда]
    end

    subgraph "Среда исполнения (LLMMode)"
        B[Распознавание речи]
        C{Цикл ReAct}
        D[Синтез речи]
    end

    subgraph "Агент (Мозг)"
        E[LLMAgent]
        F["История диалога (Память)"]
    end

    subgraph "Исполнители (Руки)"
        G[SkillExecutor]
        H["RobotTools (Набор инструментов)"]
    end

    A --> B -- Текст запроса --> C
    C -- Добавляет запрос в F --> F
    C -- Отправляет всю историю в E --> E
    E -- 1. Мысль: "Что делать дальше?" --> E
    E -- 2. Действие: Выбрать инструмент --> C
    C -- ToolCall --> G
    G -- Вызывает метод --> H
    H -- Результат (успех/ошибка) --> G
    G -- Observation --> C
    C -- Добавляет Observation в F --> F
    C -- "Продолжить цикл" --> C
    
    E -- "Задача решена, отвечаю" --> C
    C -- Финальный ответ --> D
    D --> A
```

**Описание цикла:**
1.  **Запрос:** Пользователь дает сложную задачу, например, "рассчитай прямую кинематику для текущего положения".
2.  **Мысль #1:** `LLMAgent` получает запрос. Он анализирует его и свой набор инструментов (`RobotTools`) и рассуждает: "Чтобы рассчитать прямую кинематику (`run_fk`), мне нужны углы суставов. Сначала я должен их получить".
3.  **Действие #1:** Агент решает вызвать инструмент `get_joint_positions`.
4.  **Наблюдение #1:** `SkillExecutor` выполняет `get_joint_positions` и возвращает результат (например, `[0.1, 0.2, ...]`). Этот результат добавляется в историю диалога.
5.  **Мысль #2:** `LLMAgent` снова вызывается, но теперь с полной историей. Он видит свой план и результат предыдущего действия. Он рассуждает: "Отлично, у меня есть углы суставов. Теперь я могу, наконец, вызвать `run_fk`".
6.  **Действие #2:** Агент вызывает инструмент `run_fk` с углами из предыдущего шага.
7.  **Наблюдение #2:** `SkillExecutor` выполняет `run_fk` и возвращает результат (координаты `Pose`).
8.  **Мысль #3:** Агент видит, что выполнил все шаги и получил финальный результат. Он решает, что задача выполнена, и формирует текстовый ответ для пользователя.
9.  **Ответ:** `LLMMode` получает от агента финальный текстовый ответ и озвучивает его.

---

## Справочник по модулям

-   **`main.py`**: **Точка входа.** Инициализирует все компоненты (агента, инструменты, голосовые модули) и запускает главный асинхронный цикл.
-   **`core/types.py`**: **"Словарь" проекта.** Определяет все ключевые структуры данных, используемые в системе: `Result`, `AgentMessage`, `ToolCall`, `Pose`, `Joints`, `RobotState`.
-   **`agents/`**: **"Мозг" агента.**
    -   `base.py`: Определяет интерфейс `IAgent`.
    -   `agent.py`: `LLMAgent` - ключевая реализация. Именно он:
        -   Формирует системный промпт, описывающий его задачу и инструменты.
        -   Вызывает LLM API с текущей историей диалога.
        -   Парсит ответ LLM для извлечения "мыслей" и "действий" (`tool_calls`).
-   **`tools/`**: **Набор инструментов.**
    -   `robot_tools.py`: Класс `RobotTools` агрегирует все возможности робота. **Каждый публичный `async` метод в этом классе является инструментом, доступным для LLM-агента.**
-   **`skills/`**: **"Нервная система"**.
    -   `executor.py`: `SkillExecutor` - простой, но важный компонент. Он получает `ToolCall` от агента, находит соответствующий метод в `RobotTools` по имени и выполняет его.
-   **`modes/`**: **"Режим работы" приложения.**
    -   `base.py`: Определяет базовые классы `IControlMode` и `ModeManager`.
    -   `llm_mode.py`: `LLMMode` - **главный оркестратор ReAct-цикла.** Он:
        -   Управляет "памятью" агента — списком `history`.
        -   Получает голосовой ввод и запускает цикл агента (`_run_agent_loop`).
        -   Вызывает `SkillExecutor` для выполнения действий.
        -   Обрабатывает результаты и ошибки.
        -   Управляет повторными попытками в случае неудачи.
        -   Озвучивает финальные ответы.
-   **`voice/`**: **"Органы чувств"** — все, что связано с голосом.
    -   `asr.py`: `SpeechRecognitionInput` для распознавания речи.
    -   `tts.py`: `GTTSOutput` для синтеза речи и `ConsoleOutput` для отладки.
-   **`drivers/`, `kinematics/`, `safety/`**: Низкоуровневые модули и их интерфейсы. В данном проекте содержат "заглушки" для симуляции работы реального оборудования.

---

## Использование

1.  **Скопируйте `.env.example` в `.env`** и заполните его:
    ```bash
    # Обязательно
    LLM_API_URL="https://your-llm-provider.com/v1/chat/completions"
    LLM_API_KEY="your-api-key"

    # Опционально
    LLM_MODEL="имя_модели"
    MIC_DEVICE_INDEX=0 # Если у вас несколько микрофонов
    TTS_ENABLED=true   # Установите "false" для отключения озвучки и вывода в консоль
    ```
2.  **Установите зависимости:**
    ```bash
    pip install -r requirements.txt
    ```
3.  **Запустите приложение:**
    ```bash
    python main.py
    ```
4.  Нажмите `Enter` и произнесите команду, например: "какое сейчас положение робота?" или "возьми объект в точке А и перенеси в точку Б".

---

## Расширение функционала

### Как добавить новый инструмент роботу?

Это ключевая операция для развития агента.

1.  **Шаг 1: Реализуйте метод**
    Откройте `tools/robot_tools.py` и добавьте новый асинхронный метод.

2.  **Шаг 2: Напишите подробный Docstring**
    Это **самый важный шаг**. LLM-агент будет "читать" этот docstring, чтобы понять, что делает инструмент и как его вызывать. Опишите назначение метода и каждый его параметр.
    ```python
    # в классе RobotTools
    async def new_awesome_tool(self, parameter_1: str, parameter_2: int = 10) -> Result[dict]:
        """
        Это описание того, что делает инструмент. Оно должно быть четким.
        :param parameter_1: Описание первого параметра.
        :param parameter_2: Описание второго параметра, по умолчанию 10.
        """
        # ... ваша логика ...
        print(f"Выполняю новый инструмент с {parameter_1} и {parameter_2}")
        return Result.ok({"status": "done"})
    ```

3.  **Шаг 3: Зарегистрируйте инструмент**
    Откройте `skills/executor.py` и добавьте имя вашего нового метода в словарь `_build_skill_map`.
    ```python
    # в SkillExecutor._build_skill_map
    return {
        # ... старые инструменты
        "new_awesome_tool": self.robot_tools.new_awesome_tool,
    }
    ```

**Готово!** После перезапуска агент автоматически "увидит" новый инструмент и сможет использовать его для решения задач.